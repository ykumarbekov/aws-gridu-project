{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET=ykumarbekov-534348\n"
     ]
    }
   ],
   "source": [
    "%env BUCKET=ykumarbekov-534348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# #############################\n",
    "# Loading and training data\n",
    "# Algorithm: BlazingText classification\n",
    "# #############################\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "bucket = os.environ['BUCKET']\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/datasets'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "index_to_label = {1:'spam',2:'ham'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + row[0]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "            # print('{}-{}-{}'.format(row[0],row[1],row[2]))\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))] \n",
    "    \n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    # print(transformed_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./datasets_483_982_spam-3.csv|head -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing input dataset\n",
    "train_key = '{}/{}/{}'.format(prefix, 'input/blazingtext','datasets_483_982_spam-2.csv')\n",
    "md = pd.read_csv('s3://{}/{}'.format(bucket, train_key), index_col=0)\n",
    "md1 = md.loc[md.v2.str.contains('\"') == False]\n",
    "train_data, test_data = np.split(md1.sample(frac=1, random_state=1729), [int(0.7 * len(md1))])\n",
    "train_data.to_csv('spam_input_train.csv', header=False, index=False)\n",
    "test_data.to_csv('spam_input_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__ham just sent it . so what type of food do you like ?\r\n",
      "__label__ham i cant pick the phone right now . pls send a message\r\n",
      "__label__ham get me out of this dump heap . my mom decided to come to lowes . boring .\r\n"
     ]
    }
   ],
   "source": [
    "# ! cat ./spam-input-train.csv|head -3\n",
    "# ! head -10 ./spam-input-train.csv > ./spam-input-train.csv.sample\n",
    "# ! cat ./spam_output_train.csv|grep spam|head -3\n",
    "! head -3 ./spam_output_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess('spam_input_train.csv', 'spam_output_train.csv')\n",
    "preprocess('spam_input_test.csv', 'spam_output_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://ykumarbekov-534348/sagemaker/model\n",
      "s3://ykumarbekov-534348/sagemaker/datasets/train/blazingtext\n",
      "s3://ykumarbekov-534348/sagemaker/datasets/validation/blazingtext\n"
     ]
    }
   ],
   "source": [
    "train_channel = prefix + '/train/blazingtext'\n",
    "validation_channel = prefix + '/validation/blazingtext'\n",
    "\n",
    "session.upload_data(path='spam_output_train.csv', bucket=bucket, key_prefix=train_channel)\n",
    "session.upload_data(path='spam_output_test.csv', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_output_location = 's3://{}/{}/model'.format(bucket, 'sagemaker')\n",
    "\n",
    "print(s3_output_location)\n",
    "print(s3_train_data)\n",
    "print(s3_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=session)\n",
    "\n",
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploying model\n",
    "classifier = bt_model.deploy(\n",
    "    endpoint_name='ykumarbekov-bt-endpoint',\n",
    "    model_name='ykumarbekov-bt-capstone-model',\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8328422904014587\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__spam\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9997010231018066\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__ham\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Make test predictions with endpoint\n",
    "import json\n",
    "s = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
    "     \"Bad book and useless content\"]\n",
    "ts = [' '.join(nltk.word_tokenize(s1)) for s1 in s]\n",
    "payload = {\"instances\" : ts}\n",
    "response = classifier.predict(json.dumps(payload))\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[06/14/2020 18:19:05 INFO 140650613270336] Finding and loading model\u001b[0m\n",
      "\u001b[34m[06/14/2020 18:19:05 INFO 140650613270336] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[34m[06/14/2020 18:19:05 INFO 140650613270336] Number of server workers: 4\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:05 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:05 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:05 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:05 +0000] [78] [INFO] Booting worker with pid: 78\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:05 +0000] [79] [INFO] Booting worker with pid: 79\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:06 +0000] [80] [INFO] Booting worker with pid: 80\u001b[0m\n",
      "\u001b[34m[2020-06-14 18:19:06 +0000] [81] [INFO] Booting worker with pid: 81\u001b[0m\n",
      "\u001b[32m2020-06-14T18:19:37.975:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create batch transform job\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "bucket = os.environ['BUCKET']\n",
    "output_path = 's3://{}/{}/'.format(bucket, 'logs/predictions')\n",
    "input_path = 's3://{}/{}'.format(bucket, 'logs/reviews/batch_job_test.log')\n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name='ykumarbekov-bt-capstone-model',\n",
    "    strategy='MultiRecord',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    assemble_with='Line',\n",
    "    output_path=output_path,\n",
    "    accept='application/jsonlines')\n",
    "# #####\n",
    "transformer.transform(\n",
    "    input_path, \n",
    "    content_type=\"application/jsonlines\",\n",
    "    input_filter = \"$.review\",\n",
    "    output_filter = \"$\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "# #####\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch transform job\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "bucket = os.environ['BUCKET']\n",
    "output_path = 's3://{}/{}/'.format(bucket, 'logs/predictions')\n",
    "input_path = 's3://{}/{}'.format(bucket, 'logs/reviews/batch_job_test.log')\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name='ykumarbekov-bt-capstone-model',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    assemble_with='Line',\n",
    "    output_path=output_path,\n",
    "    accept='text/csv')\n",
    "# #####\n",
    "transformer.transform(\n",
    "    input_path,     \n",
    "    content_type = \"text/csv\",\n",
    "    split_type = \"Line\", \n",
    "    input_filter = \"$[5:5]\",  \n",
    "    output_filter = \"$\"\n",
    ")\n",
    "# #####\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
